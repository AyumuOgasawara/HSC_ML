{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cfc8e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python : 3.6.13\n",
    "# Numpy: 1.17.0\n",
    "# pandas: 0.25.0\n",
    "# matplotlib: 3.1.1\n",
    "# scipy: 1.3.1\n",
    "# scikit-learn: 0.20.0\n",
    "\n",
    "import os, sys, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib as mpl\n",
    "#mpl.use('TKAgg',warn=False, force=True) #set MPL backend.\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import pickle #save/load python data objects (dictionaries/arrays)\n",
    "import time\n",
    "import itertools\n",
    "from textwrap import wrap #Long figure titles\n",
    "import multiprocessing\n",
    "#from memory_profiler import profile #profile memory\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "#ML libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn import manifold\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c516a091",
   "metadata": {},
   "source": [
    "## 使用する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dba46c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファイルのLoading/saving\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "# デバック用\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "# ファイル実行用\n",
    "    # with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "#------------------------------------------------------------------------------------------------------------ \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "#------------------------------------------------------------------------------------------------------------ \n",
    "\n",
    "# データの取得と整理\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------ \n",
    "\n",
    "\n",
    "\n",
    "# HSC天体とSDSS天体をマッチングさせ、 HSC_df と HSC_classified_df に分類する\n",
    "def separate_sources(HSC_file, SDSS_df): #HSC_file : ファイル SDSS_df : dataframe\n",
    "    \n",
    "    #HSCのデータを読み込む\n",
    "    HSC_path = 'HSC_sources/'+ HSC_file\n",
    "    HSC_df = pd.read_csv(\n",
    "        HSC_path, compression=\"gzip\", header=0, sep=\",\", quotechar='\"'\n",
    "    )\n",
    "    \n",
    "    if len(HSC_df) != 0:\n",
    "        #HSCとSDSSのデータのraとdecを取得し、単位を入れる\n",
    "        HSC_coords = SkyCoord(ra=HSC_df['ra'].to_numpy()*u.degree, dec=HSC_df['dec'].to_numpy()*u.degree)\n",
    "        SDSS_coords = SkyCoord(ra=SDSS_df['ra'].to_numpy()*u.degree, dec=SDSS_df['dec'].to_numpy()*u.degree)\n",
    "\n",
    "        #idx:インデックス d2d:天球上の距離 d3d:3次元距離\n",
    "        idx, d2d, d3d= SDSS_coords.match_to_catalog_sky(HSC_coords)\n",
    "\n",
    "        #最大距離制約を設ける\n",
    "        max_sep = 1.0 * u.arcsec\n",
    "        sep_constraint = d2d < max_sep\n",
    "\n",
    "        #マッチングした天体\n",
    "        SDSS_matches = SDSS_df[sep_constraint]\n",
    "        HSC_matches = HSC_df.iloc[idx[sep_constraint]]\n",
    "\n",
    "        #↓マッチングしてない天体を分類される天体HSC_dfに入れる↓\n",
    "        # マッチングした天体のindexをリストに変換する\n",
    "        HSC_matches_index = HSC_matches.index.tolist()\n",
    "\n",
    "        #HSC_dfからHSC_matches_index　のデータを削除したHSC_newを作成\n",
    "        HSC_df = HSC_df.drop(HSC_matches_index, axis=0)\n",
    "\n",
    "        #↓マッチングしたデータのSDSSのclassをHSCのデータに付け加えHSC_classifiedとする↓\n",
    "        #それぞれのindexをリセット\n",
    "        HSC_matches.reset_index(drop=True, inplace=True)\n",
    "        SDSS_matches.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        #HSC_Classified　にclassを追加\n",
    "        HSC_classified_df = pd.merge(HSC_matches, SDSS_matches[[\"class\"]], left_index=True, right_index=True)\n",
    "    else:\n",
    "        HSC_classified_df = HSC_df\n",
    "    \n",
    "    return HSC_df, HSC_classified_df\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# separate関数を使って分類されたデータ(HSC_classified_df)とこれから分類するデータ(HSC_new_df)に分ける\n",
    "def make_df_and_df_new(HSC_files, SDSS_df):\n",
    "    \n",
    "    # dataframeの作成\n",
    "    columns = ['# object_id', 'ra', 'dec', 'g_cmodel_mag', 'g_cmodel_magerr',\n",
    "       'r_cmodel_mag', 'r_cmodel_magerr', 'i_cmodel_mag', 'i_cmodel_magerr',\n",
    "       'z_cmodel_mag', 'z_cmodel_magerr', 'y_cmodel_mag', 'y_cmodel_magerr',\n",
    "       'g_pixelflags_saturatedcenter', 'r_pixelflags_saturatedcenter',\n",
    "       'i_pixelflags_saturatedcenter', 'z_pixelflags_saturatedcenter',\n",
    "       'y_pixelflags_saturatedcenter', 'g_pixelflags_edge',\n",
    "       'r_pixelflags_edge', 'i_pixelflags_edge', 'z_pixelflags_edge',\n",
    "       'y_pixelflags_edge', 'g_pixelflags_bad', 'r_pixelflags_bad',\n",
    "       'i_pixelflags_bad', 'z_pixelflags_bad', 'y_pixelflags_bad',\n",
    "       'g_pixelflags_bright_objectcenter', 'r_pixelflags_bright_objectcenter',\n",
    "       'i_pixelflags_bright_objectcenter', 'z_pixelflags_bright_objectcenter',\n",
    "       'y_pixelflags_bright_objectcenter', 'g_psfflux_mag', 'g_psfflux_magerr',\n",
    "       'r_psfflux_mag', 'r_psfflux_magerr', 'i_psfflux_mag',\n",
    "       'i_psfflux_magerr', 'z_psfflux_mag', 'z_psfflux_magerr',\n",
    "       'y_psfflux_mag', 'y_psfflux_magerr', 'prob_gal', 'prob_qso',\n",
    "       'prob_star']\n",
    "\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    df_news = []\n",
    "    progress = tqdm(total = len(HSC_files), unit='count')\n",
    "    \n",
    "    for HSC_file in HSC_files:\n",
    "        HSC_new_df, HSC_classified_df = separate_sources(HSC_file, SDSS_df)\n",
    "        \n",
    "        df = pd.concat([df, HSC_classified_df], axis=0, ignore_index=True, sort=False)\n",
    "        df_news.append(HSC_new_df)\n",
    "        progress.update(1)\n",
    "        \n",
    "    progress.close()\n",
    "    \n",
    "    return df, df_news\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# 分類したデータの保存\n",
    "def save_new_sources(df_news): \n",
    "    for index, df_new in enumerate(df_news):\n",
    "\n",
    "        #名前のパターンを取得し、ファイルの名前をnameに入れる\n",
    "        pattern = re.compile(r'(\\d+_\\d+_ra_\\d+)')\n",
    "        name = pattern.search(HSC_files[index]).group(1)\n",
    "\n",
    "        save_obj(df_new, 'HSC_new_sources/'+ name)\n",
    "#------------------------------------------------------------------------------------------------------------         \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "#------------------------------------------------------------------------------------------------------------ \n",
    "\n",
    "# ランダムフォレストの関数\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------ \n",
    "\n",
    "\n",
    "\n",
    "def prepare_data(df, feature_columns, train_percent=0.5):\n",
    "    all_features = df[[*feature_columns]]\n",
    "    all_classes = df['class']\n",
    "    features_train, features_test, classes_train, classes_test = train_test_split(all_features, all_classes, train_size=train_percent, test_size=(1-train_percent), random_state=0, stratify=all_classes)\n",
    "    class_names = np.unique(all_classes) #numpy.ndarray\n",
    "    feature_names = list(all_features) #list\n",
    "    return {'features_train':features_train, 'features_test':features_test, 'classes_train':classes_train, 'classes_test':classes_test, 'class_names':class_names, 'feature_names':feature_names} #return dictionary. data within dictionary are DataFrames.\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def RF_fit(df, n_estimators, n_jobs=-1):\n",
    "    print('Fitting a random forest model to the data...')\n",
    "    rfc=RandomForestClassifier(n_jobs=n_jobs, n_estimators=n_estimators, random_state=0, class_weight='balanced')\n",
    "    pipeline = Pipeline([('classification', rfc)]) # sklearn.pipeline.Pipeline\n",
    "    pipeline.fit(df['features_train'], df['classes_train'])\n",
    "    return pipeline\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def RF_classify(pipeline, data, n_jobs=-1, proba=False):\n",
    "    print('Classifying objects using random forest model...')\n",
    "    if proba==False:\n",
    "        classes_pred = pipeline.predict(data['features_test'])\n",
    "        return classes_pred\n",
    "    if proba==True:\n",
    "        classes_pred_proba = pipeline.predict_proba(data['features_test'])\n",
    "        return classes_pred_proba\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# trainingの精度を確かめる\n",
    "def metrics(df, classes_pred_all):\n",
    "    report=classification_report(df['classes_test'], classes_pred_all, target_names=np.unique(df['class_names']), digits=4, output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    cm = confusion_matrix(df['classes_test'], classes_pred_all, labels=df['class_names']) #confusion matrixの略\n",
    "    cm_df = pd.DataFrame(cm, index=df['class_names'], columns=df['class_names'])\n",
    "    return report_df, cm_df\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "def train_vs_f1score(df, place, sampleG=False):\n",
    "    train_range = [0.001, 0.003, 0.01, 0.06, 0.12, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "    \n",
    "    f1scores=[]\n",
    "    precisions=[]\n",
    "    recalls=[]\n",
    "    f1scores.append(train_range) # グラフを作る際にx軸を簡単に入力できるようtrain_rangeを0番目に入れる\n",
    "    print('Looping over these possible train percentages: {0}'.format(train_range))\n",
    "    # 初めにtrainingセットとtestセットに分ける\n",
    "    data_prep_dict_all = prepare_data(df, feature_columns, train_percent=0.5)\n",
    "    # RFのpipelineを設定する\n",
    "    pipeline = Pipeline([ ('classification', RandomForestClassifier(n_jobs=-1, n_estimators=200, random_state=0, class_weight='balanced')) ])\n",
    "\n",
    "    # training_rangeの値でdata_prep_dict_allで分けたtrain_dataをtrainし、検証する\n",
    "    for i in train_range:\n",
    "        print('train percent is: {0}'.format(i))\n",
    "        if i!=1.0:\n",
    "        # data_prep_dict_allのfeatures_trainとclasses_trainをtrainとtestデータに分ける\n",
    "            features_train, features_test, classes_train, classes_test = train_test_split(data_prep_dict_all['features_train'], data_prep_dict_all['classes_train'], train_size=i, test_size=(1-i), random_state=0, stratify=data_prep_dict_all['classes_train'])\n",
    "        if i==1.0:\n",
    "            features_train = data_prep_dict_all['features_train']\n",
    "            features_test = data_prep_dict_all['features_test']\n",
    "            classes_train = data_prep_dict_all['classes_train']\n",
    "            classes_test = data_prep_dict_all['classes_test']\n",
    "\n",
    "        print('number of sources available for training {0}'.format(len(features_train)))\n",
    "        if sampleG==True: #glaxiesの16%だけとって精度を確認するやつ\n",
    "            print('sampling galaxies to fix class imbalance...')\n",
    "            galaxy_features = features_train[classes_train == 'GALAXY']\n",
    "            quasar_features = features_train[classes_train == 'QSO']\n",
    "            star_features = features_train[classes_train == 'STAR']\n",
    "            galaxy_classes = classes_train[classes_train == 'GALAXY']\n",
    "            quasar_classes = classes_train[classes_train == 'QSO']\n",
    "            star_classes = classes_train[classes_train == 'STAR']\n",
    "            # 16%のglaxyを取るために6スキップでデータをとり、galaxy_features, galaxy_classesに入れる これで訓練用のGが13000天体くらいになる\n",
    "            galaxy_features = galaxy_features[0::6]\n",
    "            galaxy_classes = galaxy_classes[0::6]\n",
    "            # featuresをclassesを組み合わせる、一つの変数にする\n",
    "            features_train = pd.concat([galaxy_features, quasar_features, star_features])\n",
    "            classes_train = pd.concat([galaxy_classes, quasar_classes, star_classes])\n",
    "            print('Training on {0}... G: {1}, Q: {2}, S: {3}'.format(len(features_train), len(galaxy_features), len(quasar_features), len(star_features)))\n",
    "            # データをシャッフルする。\n",
    "            p = np.random.permutation(len(features_train))\n",
    "            features_train = np.array(features_train)[p]\n",
    "            classes_train = np.array(classes_train)[p]\n",
    "\n",
    "        if sampleG==False:\n",
    "            galaxy_classes = classes_train[classes_train == 'GALAXY']\n",
    "            quasar_classes = classes_train[classes_train == 'QSO']\n",
    "            star_classes = classes_train[classes_train == 'STAR']\n",
    "            print('Training on {0}... G: {1}, Q: {2}, S: {3}'.format(len(features_train), len(galaxy_classes), len(quasar_classes), len(star_classes)))\n",
    "\n",
    "        # train データをrfモデルにフィットさせる\n",
    "        pipeline.fit(features_train, classes_train)\n",
    "        # オリジナル(1番初めに分類した)のテストデータを使ってclassを分類し、精度を確認する\n",
    "        classes_pred = pipeline.predict(data_prep_dict_all['features_test'])\n",
    "        f1score = f1_score(data_prep_dict_all['classes_test'], classes_pred, average=None) #numpy.ndarray\n",
    "        precision = precision_score(data_prep_dict_all['classes_test'], classes_pred, average=None) #numpy.ndarray\n",
    "        recall = recall_score(data_prep_dict_all['classes_test'], classes_pred, average=None) #numpy.ndarray\n",
    "        print('  f1score  ', f1score) #G , S , Qの順で出力される\n",
    "        print(' precision ', precision)\n",
    "        print('    recall    ',recall)\n",
    "        f1scores.append(f1score)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        print('-'*30)\n",
    "\n",
    "    print(f1score)\n",
    "    \n",
    "    if sampleG==False:\n",
    "        save_obj(f1scores, 'HSC_ML_save/' + place +'/train_vs_f1score')\n",
    "        save_obj(precisions, 'HSC_ML_save/' + place +'/train_vs_precision')\n",
    "        save_obj(recalls, 'HSC_ML_save/' + place +'/train_vs_recall')\n",
    "    if sampleG==True:\n",
    "        save_obj(f1scores, 'HSC_ML_save/' + place +'/train_vs_f1score_sampleG')\n",
    "        save_obj(precisions, 'HSC_ML_save/' + place +'/train_vs_precision_sampleG')\n",
    "        save_obj(recalls, 'HSC_ML_save/' + place +'/train_vs_recall_sampleG')\n",
    "#------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9096eb",
   "metadata": {},
   "source": [
    "## SDSSの分類されたデータとHSCデータを読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a04f3a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SDSSデータの読み込み\n",
    "SDSS_df = load_obj('SDSS_spec_xmwise_all')\n",
    "SDSS_ra_dec_class = ['ra', 'dec', 'class']\n",
    "SDSS_df = SDSS_df[SDSS_ra_dec_class].drop_duplicates()  #重複している天体を削除 total : 3099393    G: 2209270    Q :377887   S:512236\n",
    "\n",
    "\n",
    "# HSCデータの読み込み\n",
    "HSC_files = [\"1_0_ra_20.csv.gz\", \"2_20_ra_30.csv.gz\", \"3_30_ra_40.csv.gz\",\"4_40_ra_60.csv.gz\",\"5_60_ra_120.csv.gz\",\"6_120_ra_140.csv.gz\",\"7_140_ra_160.csv.gz\",\"8_160_ra_180.csv.gz\",\"9_180_ra_200.csv.gz\",\"10_200_ra_210.csv.gz\",\"11_210_ra_220.csv.gz\",\"12_220_ra_240.csv.gz\",\"13_240_ra_300.csv.gz\",\"14_300_ra_330.csv.gz\",\"15_330_ra_360.csv.gz\"]\n",
    "df, df_news = make_df_and_df_new(HSC_files, SDSS_df) #長時間かかる\n",
    "\n",
    "# ラベルを持ってるデータと持ってないデータを保存\n",
    "save_obj(df, 'HSC_match/matched_source')\n",
    "save_new_sources(df_news)\n",
    "\n",
    "# 2回目以降のデータ読み込み\n",
    "# df = load_obj('HSC_match/matched_source')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854ed8be",
   "metadata": {},
   "source": [
    "## 天体数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e9e7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ファイルのラベル数\n",
    "'''\n",
    "\"1_0_ra_20.csv.gz\"        :     7,001,783 rows\n",
    "\"2_20_ra_30.csv.gz\"       :     4,008,557 rows \n",
    "\"3_30_ra_40.csv.gz\"       :     7,538,652 rows\n",
    "\"4_40_ra_60.csv.gz\"       :        18,150 rows\n",
    "\"5_60_ra_120.csv.gz\"      :             0 rows\n",
    "\"6_120_ra_140.csv.gz\"     :    2,917,255 rows\n",
    "\"7_140_ra_160.csv.gz\"     :    8,144,439 rows\n",
    "\"8_160_ra_180.csv.gz\"     :    9,318,738 rows\n",
    "\"9_180_ra_200.csv.gz\"     :    9,612,413 rows\n",
    "\"10_200_ra_210.csv.gz\"    :    5,536,240 rows\n",
    "\"11_210_ra_220.csv.gz\"    :    4,955,656 rows\n",
    "\"12_220_ra_240.csv.gz\"    :    4,183,360 rows\n",
    "\"13_240_ra_300.csv.gz\"    :    1,030,939 rows\n",
    "\"14_300_ra_330.csv.gz\"    :       32,787 rows\n",
    "\"15_330_ra_360.csv.gz\"    :    9,307,158 rows\n",
    "\n",
    "total : 73,606,127\n",
    "'''\n",
    "\n",
    "\n",
    "#マッチング数 47301\n",
    "'''\n",
    "\"1_0_ra_20.csv.gz\"        :       28,885 rows\n",
    "\"2_20_ra_30.csv.gz\"       :       15,763 rows\n",
    "\"3_30_ra_40.csv.gz\"       :       27,413 rows\n",
    "\"4_40_ra_60.csv.gz\"       :           85 rows\n",
    "\"5_60_ra_120.csv.gz\"      :            0 rows\n",
    "\"6_120_ra_140.csv.gz\"     :        5,421 rows\n",
    "\"7_140_ra_160.csv.gz\"     :       18,176 rows\n",
    "\"8_160_ra_180.csv.gz\"     :       21,380 rows\n",
    "\"9_180_ra_200.csv.gz\"     :       20,164 rows\n",
    "\"10_200_ra_210.csv.gz\"    :       12,850 rows\n",
    "\"11_210_ra_220.csv.gz\"    :       12,869 rows\n",
    "\"12_220_ra_240.csv.gz\"    :        9,421 rows\n",
    "\"13_240_ra_300.csv.gz\"    :        2,134 rows\n",
    "\"14_300_ra_330.csv.gz\"    :          116 rows\n",
    "\"15_330_ra_360.csv.gz\"    :       27,263 rows\n",
    "\n",
    "total : 201,940\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b5d93e",
   "metadata": {},
   "source": [
    "## モデルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8533a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータ指定\n",
    "train_percent = 0.5 # データのtrainingに充てる割合\n",
    "n_jobs=-1 # 各決定木あたりに使える最大特徴量の数\n",
    "n_estimators = 200 # 森の中に何本決定木を作るか\n",
    "\n",
    "place = 'psf_ishape_color' #特定の特徴量を保存する場所"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a023ceaa",
   "metadata": {},
   "source": [
    "## 様々な特徴のモデルを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3033dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 特徴量[psf , resolved_i, ishape]\n",
    "\n",
    "# df['resolved_i'] = np.sqrt((df.i_psfflux_mag - df.i_cmodel_mag)**2)\n",
    "# df = df.dropna(subset=['ishape']) # ishapeの欠損値をなくす  447天体欠損　合計201,493天体\n",
    "\n",
    "# psf = ['g_psfflux_mag','r_psfflux_mag', 'i_psfflux_mag', 'z_psfflux_mag', 'y_psfflux_mag']\n",
    "# feature_columns = psf + ['resolved_i'] + ['ishape']\n",
    "# feature_labels = ['g','r','i','z','y', '$\\mathrm{resolved}_\\mathrm{i}$ ', '$\\mathrm{ishape}$']\n",
    "\n",
    "# print('features used are:')\n",
    "# print(df[feature_columns].columns)\n",
    "\n",
    "# data_prep_dict_all = prepare_data(df, feature_columns, train_percent)\n",
    "# pipeline = RF_fit(data_prep_dict_all, n_estimators, n_jobs=-1)\n",
    "# classes_pred_all = RF_classify(pipeline, data_prep_dict_all, n_jobs=-1, proba=False) #<class 'numpy.ndarray'>\n",
    "# classes_pred_all_proba = RF_classify(pipeline, data_prep_dict_all, n_jobs=-1, proba=True) #<class 'numpy.ndarray'>\n",
    "# report_df, cm_df = metrics(data_prep_dict_all, classes_pred_all)\n",
    "\n",
    "# report_df #違うセルでの実行推奨\n",
    "# cm_df #違うセルでの実行推奨\n",
    "\n",
    "\n",
    "# save_obj(pipeline, 'HSC_ML_save/psf_resolved_i_ishape/rf_pipeline') # pipelineのclassificationが入ってる\n",
    "# save_obj(data_prep_dict_all, 'HSC_ML_save/psf_resolved_i_ishape/data_prep_dict_all') #訓練データとトレーニングデータに分けたもの\n",
    "# save_obj(classes_pred_all, 'HSC_ML_save/psf_resolved_i_ishape/classes_pred_all') #テストデータをモデルにフィットさせ、返ってきたテストデータのクラス\n",
    "# save_obj(classes_pred_all_proba,'HSC_ML_save/psf_resolved_i_ishape/classes_pred_all_proba') #テストデータのクラスの分類確率\n",
    "\n",
    "# df_predclass = pd.DataFrame(classes_pred_all, index=data_prep_dict_all['features_test'].index, columns=['class_pred'])\n",
    "# df = df.join(df_predclass, how='left')\n",
    "\n",
    "# df_proba = pd.DataFrame(classes_pred_all_proba, index=data_prep_dict_all['features_test'].index, columns=['prob_g', 'prob_q', 'prob_s'])\n",
    "# # Append probabilities to the original df for test data:\n",
    "# df = df.join(df_proba, how='left')\n",
    "# df['prob_best'] = df[['prob_g', 'prob_q', 'prob_s']].max(axis=1) # max(axis=1)で水平方向の最大を求めることで、prob_bestに、最大値が入る\n",
    "\n",
    "\n",
    "# save_obj(df, 'HSC_ML_save/psf_resolved_i_ishape/df_spec_classprobs')\n",
    "# #------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1609a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 特徴量[psf , resolved_i]\n",
    "\n",
    "# df['resolved_i'] = np.sqrt((df.i_psfflux_mag - df.i_cmodel_mag)**2)\n",
    "# psf = ['g_psfflux_mag','r_psfflux_mag', 'i_psfflux_mag', 'z_psfflux_mag', 'y_psfflux_mag']\n",
    "# feature_columns = psf + ['resolved_i']\n",
    "# feature_labels = ['g','r','i','z','y', '$\\mathrm{resolved}_\\mathrm{i}$ ']\n",
    "\n",
    "# print('features used are:')\n",
    "# print(df[feature_columns].columns)\n",
    "\n",
    "# data_prep_dict_all = prepare_data(df, feature_columns, train_percent)\n",
    "# pipeline = RF_fit(data_prep_dict_all, n_estimators, n_jobs=-1)\n",
    "# classes_pred_all = RF_classify(pipeline, data_prep_dict_all, n_jobs=-1, proba=False) #<class 'numpy.ndarray'>\n",
    "# classes_pred_all_proba = RF_classify(pipeline, data_prep_dict_all, n_jobs=-1, proba=True) #<class 'numpy.ndarray'>\n",
    "# report_df, cm_df = metrics(data_prep_dict_all, classes_pred_all)\n",
    "\n",
    "# report_df #違うセルでの実行推奨\n",
    "# cm_df #違うセルでの実行推奨\n",
    "\n",
    "# save_obj(pipeline, 'HSC_ML_save/psf_resolved_i/rf_pipeline') # pipelineのclassificationが入ってる\n",
    "# save_obj(data_prep_dict_all, 'HSC_ML_save/psf_resolved_i/data_prep_dict_all') #訓練データとトレーニングデータに分けたもの\n",
    "# save_obj(classes_pred_all, 'HSC_ML_save/psf_resolved_i/classes_pred_all') #テストデータをモデルにフィットさせ、返ってきたテストデータのクラス\n",
    "# save_obj(classes_pred_all_proba,'HSC_ML_save/psf_resolved_i/classes_pred_all_proba') #テストデータのクラスの分類確率\n",
    "\n",
    "# df_predclass = pd.DataFrame(classes_pred_all, index=data_prep_dict_all['features_test'].index, columns=['class_pred'])\n",
    "# df = df.join(df_predclass, how='left')\n",
    "\n",
    "# df_proba = pd.DataFrame(classes_pred_all_proba, index=data_prep_dict_all['features_test'].index, columns=['prob_g', 'prob_q', 'prob_s'])\n",
    "# # Append probabilities to the original df for test data:\n",
    "# df = df.join(df_proba, how='left')\n",
    "# df['prob_best'] = df[['prob_g', 'prob_q', 'prob_s']].max(axis=1) # max(axis=1)で水平方向の最大を求めることで、prob_bestに、最大値が入る\n",
    "\n",
    "# save_obj(df, 'HSC_ML_save/psf_resolved_i/df_spec_classprobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31493c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 特徴量[psf , ishape]\n",
    "\n",
    "# # ishapeの欠損値をなくす  447天体欠損　合計201,493天体\n",
    "# df = df.dropna(subset=['ishape'])\n",
    "\n",
    "# psf = ['g_psfflux_mag','r_psfflux_mag', 'i_psfflux_mag', 'z_psfflux_mag', 'y_psfflux_mag']\n",
    "# feature_columns = psf + ['ishape']\n",
    "# feature_labels = ['g','r','i','z','y', '$\\mathrm{ishape}$']\n",
    "\n",
    "# print('features used are:')\n",
    "# print(df[feature_columns].columns)\n",
    "\n",
    "# data_prep_dict_all = prepare_data(df, feature_columns, train_percent)\n",
    "# pipeline = RF_fit(data_prep_dict_all, n_estimators, n_jobs=-1)\n",
    "# classes_pred_all = RF_classify(pipeline, data_prep_dict_all, n_jobs=-1, proba=False) #<class 'numpy.ndarray'>\n",
    "# classes_pred_all_proba = RF_classify(pipeline, data_prep_dict_all, n_jobs=-1, proba=True) #<class 'numpy.ndarray'>\n",
    "# report_df, cm_df = metrics(data_prep_dict_all, classes_pred_all)\n",
    "\n",
    "# report_df #違うセルでの実行推奨\n",
    "# cm_df #違うセルでの実行推奨\n",
    "\n",
    "# save_obj(pipeline, 'HSC_ML_save/psf_ishape/rf_pipeline') # pipelineのclassificationが入ってる\n",
    "# save_obj(data_prep_dict_all, 'HSC_ML_save/psf_ishape/data_prep_dict_all') #訓練データとトレーニングデータに分けたもの\n",
    "# save_obj(classes_pred_all, 'HSC_ML_save/psf_ishape/classes_pred_all') #テストデータをモデルにフィットさせ、返ってきたテストデータのクラス\n",
    "# save_obj(classes_pred_all_proba,'HSC_ML_save/psf_ishape/classes_pred_all_proba') #テストデータのクラスの分類確率\n",
    "\n",
    "# df_predclass = pd.DataFrame(classes_pred_all, index=data_prep_dict_all['features_test'].index, columns=['class_pred'])\n",
    "# df = df.join(df_predclass, how='left')\n",
    "\n",
    "# df_proba = pd.DataFrame(classes_pred_all_proba, index=data_prep_dict_all['features_test'].index, columns=['prob_g', 'prob_q', 'prob_s'])\n",
    "# # Append probabilities to the original df for test data:\n",
    "# df = df.join(df_proba, how='left')\n",
    "# df['prob_best'] = df[['prob_g', 'prob_q', 'prob_s']].max(axis=1) # max(axis=1)で水平方向の最大を求めることで、prob_bestに、最大値が入る\n",
    "\n",
    "# save_obj(df, 'HSC_ML_save/psf_ishape/df_spec_classprobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1725e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 特徴量[cmodel , ishape]\n",
    "\n",
    "# # ishapeの欠損値をなくす  447天体欠損　合計201,493天体\n",
    "# df = df.dropna(subset=['ishape'])\n",
    "\n",
    "# # cmodel magnitude\n",
    "# cmodel = ['g_cmodel_mag', 'r_cmodel_mag', 'i_cmodel_mag', 'z_cmodel_mag', 'y_cmodel_mag']\n",
    "# feature_columns = cmodel + ['ishape']\n",
    "# feature_labels = ['g','r','i','z','y', '$\\mathrm{ishape}$']\n",
    "\n",
    "# print('features used are:')\n",
    "# print(df[feature_columns].columns)\n",
    "\n",
    "# data_prep_dict_all = prepare_data(df, feature_columns, train_percent)\n",
    "# pipeline = RF_fit(data_prep_dict_all, n_estimators, n_jobs=-1)\n",
    "# classes_pred_all = RF_classify(pipeline, data_prep_dict_all, n_jobs=-1, proba=False) #<class 'numpy.ndarray'>\n",
    "# classes_pred_all_proba = RF_classify(pipeline, data_prep_dict_all, n_jobs=-1, proba=True) #<class 'numpy.ndarray'>\n",
    "# report_df, cm_df = metrics(data_prep_dict_all, classes_pred_all)\n",
    "\n",
    "# report_df #違うセルでの実行推奨\n",
    "# cm_df #違うセルでの実行推奨\n",
    "\n",
    "# # psf+resolved_i+ishape\n",
    "# save_obj(pipeline, 'HSC_ML_save/cmodel_ishape/rf_pipeline') # pipelineのclassificationが入ってる\n",
    "# save_obj(data_prep_dict_all, 'HSC_ML_save/cmodel_ishape/data_prep_dict_all') #訓練データとトレーニングデータに分けたもの\n",
    "# save_obj(classes_pred_all, 'HSC_ML_save/cmodel_ishape/classes_pred_all') #テストデータをモデルにフィットさせ、返ってきたテストデータのクラス\n",
    "# save_obj(classes_pred_all_proba,'HSC_ML_save/cmodel_ishape/classes_pred_all_proba') #テストデータのクラスの分類確率\n",
    "\n",
    "# df_predclass = pd.DataFrame(classes_pred_all, index=data_prep_dict_all['features_test'].index, columns=['class_pred'])\n",
    "# df = df.join(df_predclass, how='left')\n",
    "\n",
    "# df_proba = pd.DataFrame(classes_pred_all_proba, index=data_prep_dict_all['features_test'].index, columns=['prob_g', 'prob_q', 'prob_s'])\n",
    "# # Append probabilities to the original df for test data:\n",
    "# df = df.join(df_proba, how='left')\n",
    "# df['prob_best'] = df[['prob_g', 'prob_q', 'prob_s']].max(axis=1) # max(axis=1)で水平方向の最大を求めることで、prob_bestに、最大値が入る\n",
    "\n",
    "# save_obj(df, 'HSC_ML_save/cmodel_ishape/df_spec_classprobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72accdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 特徴量[psf]\n",
    "\n",
    "# psf = ['g_psfflux_mag','r_psfflux_mag', 'i_psfflux_mag', 'z_psfflux_mag', 'y_psfflux_mag']\n",
    "# feature_columns = psf\n",
    "# feature_labels = ['g','r','i','z','y']\n",
    "\n",
    "# print('features used are:')\n",
    "# print(df[feature_columns].columns)\n",
    "\n",
    "# data_prep_dict_all = prepare_data(df, feature_columns, train_percent)\n",
    "# pipeline = RF_fit(data_prep_dict_all, n_estimators, n_jobs=-1)\n",
    "# classes_pred_all = RF_classify(pipeline, data_prep_dict_all, n_jobs=-1, proba=False) #<class 'numpy.ndarray'>\n",
    "# classes_pred_all_proba = RF_classify(pipeline, data_prep_dict_all, n_jobs=-1, proba=True) #<class 'numpy.ndarray'>\n",
    "# report_df, cm_df = metrics(data_prep_dict_all, classes_pred_all)\n",
    "\n",
    "# report_df #違うセルでの実行推奨\n",
    "# cm_df #違うセルでの実行推奨\n",
    "\n",
    "# save_obj(pipeline, 'HSC_ML_save/psf/rf_pipeline') # pipelineのclassificationが入ってる\n",
    "# save_obj(data_prep_dict_all, 'HSC_ML_save/psf/data_prep_dict_all') #訓練データとトレーニングデータに分けたもの\n",
    "# save_obj(classes_pred_all, 'HSC_ML_save/psf/classes_pred_all') #テストデータをモデルにフィットさせ、返ってきたテストデータのクラス\n",
    "# save_obj(classes_pred_all_proba,'HSC_ML_save/psf/classes_pred_all_proba') #テストデータのクラスの分類確率\n",
    "\n",
    "# df_predclass = pd.DataFrame(classes_pred_all, index=data_prep_dict_all['features_test'].index, columns=['class_pred'])\n",
    "# df = df.join(df_predclass, how='left')\n",
    "\n",
    "# df_proba = pd.DataFrame(classes_pred_all_proba, index=data_prep_dict_all['features_test'].index, columns=['prob_g', 'prob_q', 'prob_s'])\n",
    "# # Append probabilities to the original df for test data:\n",
    "# df = df.join(df_proba, how='left')\n",
    "# df['prob_best'] = df[['prob_g', 'prob_q', 'prob_s']].max(axis=1) # max(axis=1)で水平方向の最大を求めることで、prob_bestに、最大値が入る\n",
    "\n",
    "# save_obj(df, 'HSC_ML_save/psf/df_spec_classprobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d241fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 特徴量[ishape]\n",
    "\n",
    "# # ishapeの欠損値をなくす  447天体欠損　合計201,493天体\n",
    "# df = df.dropna(subset=['ishape'])\n",
    "# feature_columns = ['ishape']\n",
    "# feature_labels = ['ishape']\n",
    "\n",
    "# print('features used are:')\n",
    "# print(df[feature_columns].columns)\n",
    "\n",
    "# data_prep_dict_all = prepare_data(df, feature_columns, train_percent)\n",
    "# pipeline = RF_fit(data_prep_dict_all, n_estimators, n_jobs=-1)\n",
    "# classes_pred_all = RF_classify(pipeline, data_prep_dict_all, n_jobs=-1, proba=False) #<class 'numpy.ndarray'>\n",
    "# classes_pred_all_proba = RF_classify(pipeline, data_prep_dict_all, n_jobs=-1, proba=True) #<class 'numpy.ndarray'>\n",
    "# report_df, cm_df = metrics(data_prep_dict_all, classes_pred_all)\n",
    "\n",
    "# report_df #違うセルでの実行推奨\n",
    "# cm_df #違うセルでの実行推奨\n",
    "\n",
    "# save_obj(pipeline, 'HSC_ML_save/ishape/rf_pipeline') # pipelineのclassificationが入ってる\n",
    "# save_obj(data_prep_dict_all, 'HSC_ML_save/ishape/data_prep_dict_all') #訓練データとトレーニングデータに分けたもの\n",
    "# save_obj(classes_pred_all, 'HSC_ML_save/ishape/classes_pred_all') #テストデータをモデルにフィットさせ、返ってきたテストデータのクラス\n",
    "# save_obj(classes_pred_all_proba,'HSC_ML_save/ishape/classes_pred_all_proba') #テストデータのクラスの分類確率\n",
    "\n",
    "# df_predclass = pd.DataFrame(classes_pred_all, index=data_prep_dict_all['features_test'].index, columns=['class_pred'])\n",
    "# df = df.join(df_predclass, how='left')\n",
    "\n",
    "# df_proba = pd.DataFrame(classes_pred_all_proba, index=data_prep_dict_all['features_test'].index, columns=['prob_g', 'prob_q', 'prob_s'])\n",
    "# # Append probabilities to the original df for test data:\n",
    "# df = df.join(df_proba, how='left')\n",
    "# df['prob_best'] = df[['prob_g', 'prob_q', 'prob_s']].max(axis=1) # max(axis=1)で水平方向の最大を求めることで、prob_bestに、最大値が入る\n",
    "\n",
    "# # psf+resolved_i+ishape\n",
    "# save_obj(df, 'HSC_ML_save/ishape/df_spec_classprobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9713ca2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features used are:\n",
      "Index(['g_psfflux_mag', 'r_psfflux_mag', 'i_psfflux_mag', 'z_psfflux_mag',\n",
      "       'y_psfflux_mag', 'ishape', 'g-r', 'r-i', 'i-z', 'z-y'],\n",
      "      dtype='object')\n",
      "Fitting a random forest model to the data...\n",
      "Classifying objects using random forest model...\n",
      "Classifying objects using random forest model...\n"
     ]
    }
   ],
   "source": [
    "# 特徴量[psf , ishape, color] 1番精度が高かった\n",
    "\n",
    "# ishapeの欠損値をなくす  447天体欠損　合計201,493天体\n",
    "df = df.dropna(subset=['ishape'])\n",
    "\n",
    "df['g-r'] = df.g_psfflux_mag - df.r_psfflux_mag\n",
    "df['r-i'] = df.r_psfflux_mag - df.i_psfflux_mag\n",
    "df['i-z'] = df.i_psfflux_mag - df.z_psfflux_mag\n",
    "df['z-y'] = df.z_psfflux_mag - df.y_psfflux_mag\n",
    "\n",
    "psf = ['g_psfflux_mag','r_psfflux_mag', 'i_psfflux_mag', 'z_psfflux_mag', 'y_psfflux_mag']\n",
    "color = ['g-r', 'r-i', 'i-z', 'z-y']\n",
    "feature_columns = psf + ['ishape'] + color\n",
    "feature_labels = ['g','r','i','z','y', '$\\mathrm{ishape}$', 'g-r', 'r-i', 'i-z', 'z-y']\n",
    "place = 'psf_ishape_color'\n",
    "\n",
    "print('features used are:')\n",
    "print(df[feature_columns].columns)\n",
    "\n",
    "data_prep_dict_all = prepare_data(df, feature_columns, train_percent)\n",
    "pipeline = RF_fit(data_prep_dict_all, n_estimators, n_jobs=-1)\n",
    "classes_pred_all = RF_classify(pipeline, data_prep_dict_all, n_jobs=-1, proba=False) #<class 'numpy.ndarray'>\n",
    "classes_pred_all_proba = RF_classify(pipeline, data_prep_dict_all, n_jobs=-1, proba=True) #<class 'numpy.ndarray'>\n",
    "report_df, cm_df = metrics(data_prep_dict_all, classes_pred_all)\n",
    "\n",
    "report_df #違うセルでの実行推奨\n",
    "cm_df #違うセルでの実行推奨\n",
    "\n",
    "save_obj(pipeline, 'HSC_ML_save/psf_ishape_color/rf_pipeline') # pipelineのclassificationが入ってる\n",
    "save_obj(data_prep_dict_all, 'HSC_ML_save/psf_ishape_color/data_prep_dict_all') #訓練データとトレーニングデータに分けたもの\n",
    "save_obj(classes_pred_all, 'HSC_ML_save/psf_ishape_color/classes_pred_all') #テストデータをモデルにフィットさせ、返ってきたテストデータのクラス\n",
    "save_obj(classes_pred_all_proba,'HSC_ML_save/psf_ishape_color/classes_pred_all_proba') #テストデータのクラスの分類確率\n",
    "\n",
    "df_predclass = pd.DataFrame(classes_pred_all, index=data_prep_dict_all['features_test'].index, columns=['class_pred'])\n",
    "df = df.join(df_predclass, how='left')\n",
    "\n",
    "df_proba = pd.DataFrame(classes_pred_all_proba, index=data_prep_dict_all['features_test'].index, columns=['prob_g', 'prob_q', 'prob_s'])\n",
    "# Append probabilities to the original df for test data:\n",
    "df = df.join(df_proba, how='left')\n",
    "df['prob_best'] = df[['prob_g', 'prob_q', 'prob_s']].max(axis=1) # max(axis=1)で水平方向の最大を求めることで、prob_bestに、最大値が入る\n",
    "\n",
    "save_obj(df, 'HSC_ML_save/psf_ishape_color/df_spec_classprobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "423aece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 特徴量[cmodel , ishape, color]\n",
    "\n",
    "# # ishapeの欠損値をなくす  447天体欠損　合計201,493天体\n",
    "# df = df.dropna(subset=['ishape'])\n",
    "\n",
    "# df['g-r'] = df.g_psfflux_mag - df.r_psfflux_mag\n",
    "# df['r-i'] = df.r_psfflux_mag - df.i_psfflux_mag\n",
    "# df['i-z'] = df.i_psfflux_mag - df.z_psfflux_mag\n",
    "# df['z-y'] = df.z_psfflux_mag - df.y_psfflux_mag\n",
    "\n",
    "# cmodel = ['g_cmodel_mag', 'r_cmodel_mag', 'i_cmodel_mag', 'z_cmodel_mag', 'y_cmodel_mag']\n",
    "# color = ['g-r', 'r-i', 'i-z', 'z-y']\n",
    "# feature_columns = cmodel + ['ishape'] + color\n",
    "# feature_labels = ['g','r','i','z','y', '$\\mathrm{ishape}$', 'g-r', 'r-i', 'i-z', 'z-y']\n",
    "# place = 'cmodel_ishape_color'\n",
    "\n",
    "# print('features used are:')\n",
    "# print(df[feature_columns].columns)\n",
    "\n",
    "# data_prep_dict_all = prepare_data(df, feature_columns, train_percent)\n",
    "# pipeline = RF_fit(data_prep_dict_all, n_estimators, n_jobs=-1)\n",
    "# classes_pred_all = RF_classify(pipeline, data_prep_dict_all, n_jobs=-1, proba=False) #<class 'numpy.ndarray'>\n",
    "# classes_pred_all_proba = RF_classify(pipeline, data_prep_dict_all, n_jobs=-1, proba=True) #<class 'numpy.ndarray'>\n",
    "# report_df, cm_df = metrics(data_prep_dict_all, classes_pred_all)\n",
    "\n",
    "# report_df #違うセルでの実行推奨\n",
    "# cm_df #違うセルでの実行推奨\n",
    "\n",
    "# save_obj(pipeline, 'HSC_ML_save/'+place+'/rf_pipeline') # pipelineのclassificationが入ってる\n",
    "# save_obj(data_prep_dict_all, 'HSC_ML_save/'+place+'/data_prep_dict_all') #訓練データとトレーニングデータに分けたもの\n",
    "# save_obj(classes_pred_all, 'HSC_ML_save/'+place+'/classes_pred_all') #テストデータをモデルにフィットさせ、返ってきたテストデータのクラス\n",
    "# save_obj(classes_pred_all_proba,'HSC_ML_save/'+place+'/classes_pred_all_proba') #テストデータのクラスの分類確率\n",
    "\n",
    "# df_predclass = pd.DataFrame(classes_pred_all, index=data_prep_dict_all['features_test'].index, columns=['class_pred'])\n",
    "# df = df.join(df_predclass, how='left')\n",
    "\n",
    "# df_proba = pd.DataFrame(classes_pred_all_proba, index=data_prep_dict_all['features_test'].index, columns=['prob_g', 'prob_q', 'prob_s'])\n",
    "# # Append probabilities to the original df for test data:\n",
    "# df = df.join(df_proba, how='left')\n",
    "# df['prob_best'] = df[['prob_g', 'prob_q', 'prob_s']].max(axis=1) # max(axis=1)で水平方向の最大を求めることで、prob_bestに、最大値が入る\n",
    "\n",
    "# save_obj(df, 'HSC_ML_save/'+place+'/df_spec_classprobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9d639a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 特徴量[ishape, color]\n",
    "\n",
    "# # ishapeの欠損値をなくす  447天体欠損　合計201,493天体\n",
    "# df = df.dropna(subset=['ishape'])\n",
    "\n",
    "# df['g-r'] = df.g_psfflux_mag - df.r_psfflux_mag\n",
    "# df['r-i'] = df.r_psfflux_mag - df.i_psfflux_mag\n",
    "# df['i-z'] = df.i_psfflux_mag - df.z_psfflux_mag\n",
    "# df['z-y'] = df.z_psfflux_mag - df.y_psfflux_mag\n",
    "\n",
    "# color = ['g-r', 'r-i', 'i-z', 'z-y']\n",
    "# feature_columns = ['ishape'] + color\n",
    "# feature_labels = ['$\\mathrm{ishape}$', 'g-r', 'r-i', 'i-z', 'z-y']\n",
    "\n",
    "# print('features used are:')\n",
    "# print(df[feature_columns].columns)\n",
    "\n",
    "# data_prep_dict_all = prepare_data(df, feature_columns, train_percent)\n",
    "# pipeline = RF_fit(data_prep_dict_all, n_estimators, n_jobs=-1)\n",
    "# classes_pred_all = RF_classify(pipeline, data_prep_dict_all, n_jobs=-1, proba=False) #<class 'numpy.ndarray'>\n",
    "# classes_pred_all_proba = RF_classify(pipeline, data_prep_dict_all, n_jobs=-1, proba=True) #<class 'numpy.ndarray'>\n",
    "# report_df, cm_df = metrics(data_prep_dict_all, classes_pred_all)\n",
    "\n",
    "# save_obj(pipeline, 'HSC_ML_save/ishape_color/rf_pipeline') # pipelineのclassificationが入ってる\n",
    "# save_obj(data_prep_dict_all, 'HSC_ML_save/ishape_color/data_prep_dict_all') #訓練データとトレーニングデータに分けたもの\n",
    "# save_obj(classes_pred_all, 'HSC_ML_save/ishape_color/classes_pred_all') #テストデータをモデルにフィットさせ、返ってきたテストデータのクラス\n",
    "# save_obj(classes_pred_all_proba,'HSC_ML_save/ishape_color/classes_pred_all_proba') #テストデータのクラスの分類確率\n",
    "\n",
    "# df_predclass = pd.DataFrame(classes_pred_all, index=data_prep_dict_all['features_test'].index, columns=['class_pred'])\n",
    "# df = df.join(df_predclass, how='left')\n",
    "\n",
    "# df_proba = pd.DataFrame(classes_pred_all_proba, index=data_prep_dict_all['features_test'].index, columns=['prob_g', 'prob_q', 'prob_s'])\n",
    "# # Append probabilities to the original df for test data:\n",
    "# df = df.join(df_proba, how='left')\n",
    "# df['prob_best'] = df[['prob_g', 'prob_q', 'prob_s']].max(axis=1) # max(axis=1)で水平方向の最大を求めることで、prob_bestに、最大値が入る\n",
    "\n",
    "# save_obj(df, 'HSC_ML_save/ishape_color/df_spec_classprobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a091a5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looping over these possible train percentages: [0.001, 0.003, 0.01, 0.06, 0.12, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
      "train percent is: 0.001\n",
      "number of sources available for training 100\n",
      "sampling galaxies to fix class imbalance...\n",
      "Training on 32... G: 14, Q: 14, S: 4\n",
      "  f1score   [0.95320268 0.74790298 0.70767949]\n",
      " precision  [0.96126144 0.68812444 0.86372881]\n",
      "    recall     [0.94527792 0.81905576 0.59938838]\n",
      "------------------------------\n",
      "train percent is: 0.003\n",
      "number of sources available for training 302\n",
      "sampling galaxies to fix class imbalance...\n",
      "Training on 97... G: 42, Q: 42, S: 13\n",
      "  f1score   [0.98396951 0.88924969 0.83882192]\n",
      " precision  [0.98641006 0.86353573 0.88909378]\n",
      "    recall     [0.981541   0.91654205 0.79393084]\n",
      "------------------------------\n",
      "train percent is: 0.01\n",
      "number of sources available for training 1007\n",
      "sampling galaxies to fix class imbalance...\n",
      "Training on 321... G: 138, Q: 140, S: 43\n",
      "  f1score   [0.98797024 0.90944868 0.86939712]\n",
      " precision  [0.98885851 0.89768313 0.89369101]\n",
      "    recall     [0.98708355 0.92152674 0.84638908]\n",
      "------------------------------\n",
      "train percent is: 0.06\n",
      "number of sources available for training 6044\n",
      "sampling galaxies to fix class imbalance...\n",
      "Training on 1922... G: 825, Q: 842, S: 255\n",
      "  f1score   [0.98718268 0.91138804 0.90169575]\n",
      " precision  [0.99030927 0.88442985 0.94391562]\n",
      "    recall     [0.98407578 0.9400413  0.86309104]\n",
      "------------------------------\n",
      "train percent is: 0.12\n",
      "number of sources available for training 12089\n",
      "sampling galaxies to fix class imbalance...\n",
      "Training on 3844... G: 1649, Q: 1685, S: 510\n",
      "  f1score   [0.98745386 0.91429362 0.9120098 ]\n",
      " precision  [0.99030251 0.88921793 0.95190586]\n",
      "    recall     [0.98462154 0.94082461 0.87532345]\n",
      "------------------------------\n",
      "train percent is: 0.2\n",
      "number of sources available for training 20149\n",
      "sampling galaxies to fix class imbalance...\n",
      "Training on 6408... G: 2749, Q: 2808, S: 851\n",
      "  f1score   [0.98753536 0.91599253 0.91824514]\n",
      " precision  [0.99067519 0.89014312 0.95549339]\n",
      "    recall     [0.98441536 0.94338816 0.88379205]\n",
      "------------------------------\n",
      "train percent is: 0.4\n",
      "number of sources available for training 40298\n",
      "sampling galaxies to fix class imbalance...\n",
      "Training on 12815... G: 5497, Q: 5617, S: 1701\n",
      "  f1score   [0.98778143 0.91965706 0.9241614 ]\n",
      " precision  [0.99119518 0.89370423 0.95599698]\n",
      "    recall     [0.98439111 0.94716229 0.89437779]\n",
      "------------------------------\n",
      "train percent is: 0.6\n",
      "number of sources available for training 60447\n",
      "sampling galaxies to fix class imbalance...\n",
      "Training on 19222... G: 8246, Q: 8425, S: 2551\n",
      "  f1score   [0.98685934 0.91538118 0.92912621]\n",
      " precision  [0.99144368 0.88436006 0.95963901]\n",
      "    recall     [0.9823172  0.94865769 0.900494  ]\n",
      "------------------------------\n",
      "train percent is: 0.8\n",
      "number of sources available for training 80596\n",
      "sampling galaxies to fix class imbalance...\n",
      "Training on 25629... G: 10994, Q: 11233, S: 3402\n",
      "  f1score   [0.98715239 0.91746905 0.93255842]\n",
      " precision  [0.99168931 0.88714504 0.96082834]\n",
      "    recall     [0.98265679 0.94993947 0.90590449]\n",
      "------------------------------\n",
      "train percent is: 1.0\n",
      "number of sources available for training 100746\n",
      "sampling galaxies to fix class imbalance...\n",
      "Training on 32036... G: 13742, Q: 14042, S: 4252\n",
      "  f1score   [0.98744755 0.91954419 0.93483527]\n",
      " precision  [0.99176638 0.89009597 0.96354557]\n",
      "    recall     [0.98316617 0.95100762 0.9077864 ]\n",
      "------------------------------\n",
      "[0.98744755 0.91954419 0.93483527]\n",
      "Looping over these possible train percentages: [0.001, 0.003, 0.01, 0.06, 0.12, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
      "train percent is: 0.001\n",
      "number of sources available for training 100\n",
      "Training on 100... G: 82, Q: 14, S: 4\n",
      "  f1score   [0.96672126 0.78786638 0.63748697]\n",
      " precision  [0.93678893 0.92662494 0.86820762]\n",
      "    recall     [0.99862952 0.68525244 0.5036462 ]\n",
      "------------------------------\n",
      "train percent is: 0.003\n",
      "number of sources available for training 302\n",
      "Training on 302... G: 247, Q: 42, S: 13\n",
      "  f1score   [0.98538189 0.89318089 0.84115523]\n",
      " precision  [0.97482732 0.92960567 0.93067047]\n",
      "    recall     [0.99616751 0.85950296 0.76734886]\n",
      "------------------------------\n",
      "train percent is: 0.01\n",
      "number of sources available for training 1007\n",
      "Training on 1007... G: 824, Q: 140, S: 43\n",
      "  f1score   [0.98948549 0.91339614 0.88113092]\n",
      " precision  [0.9837915  0.9335316  0.92276004]\n",
      "    recall     [0.99524578 0.89411094 0.84309574]\n",
      "------------------------------\n",
      "train percent is: 0.06\n",
      "number of sources available for training 6044\n",
      "Training on 6044... G: 4947, Q: 842, S: 255\n",
      "  f1score   [0.99004783 0.92480528 0.90685238]\n",
      " precision  [0.98596327 0.93233464 0.96200528]\n",
      "    recall     [0.99416637 0.91739657 0.85768055]\n",
      "------------------------------\n",
      "train percent is: 0.12\n",
      "number of sources available for training 12089\n",
      "Training on 12089... G: 9894, Q: 1685, S: 510\n",
      "  f1score   [0.99041626 0.92903412 0.91334895]\n",
      " precision  [0.98632427 0.93910513 0.95934749]\n",
      "    recall     [0.99454235 0.91917681 0.87155963]\n",
      "------------------------------\n",
      "train percent is: 0.2\n",
      "number of sources available for training 20149\n",
      "Training on 20149... G: 16490, Q: 2808, S: 851\n",
      "  f1score   [0.99051254 0.93123333 0.9199853 ]\n",
      " precision  [0.9864556  0.94278625 0.9601023 ]\n",
      "    recall     [0.99460299 0.91996012 0.88308633]\n",
      "------------------------------\n",
      "train percent is: 0.4\n",
      "number of sources available for training 40298\n",
      "Training on 40298... G: 32980, Q: 5617, S: 1701\n",
      "  f1score   [0.99076994 0.93318453 0.924847  ]\n",
      " precision  [0.98695438 0.94364762 0.96402143]\n",
      "    recall     [0.99461511 0.92295094 0.88873206]\n",
      "------------------------------\n",
      "train percent is: 0.6\n",
      "number of sources available for training 60447\n",
      "Training on 60447... G: 49471, Q: 8425, S: 2551\n",
      "  f1score   [0.99079931 0.93477477 0.92925849]\n",
      " precision  [0.98707239 0.94623185 0.96314992]\n",
      "    recall     [0.99455447 0.92359183 0.89767114]\n",
      "------------------------------\n",
      "train percent is: 0.8\n",
      "number of sources available for training 80596\n",
      "Training on 80596... G: 65961, Q: 11233, S: 3402\n",
      "  f1score   [0.99098054 0.93636953 0.93239128]\n",
      " precision  [0.98725294 0.94770622 0.96690248]\n",
      "    recall     [0.9947364  0.92530086 0.90025876]\n",
      "------------------------------\n",
      "train percent is: 1.0\n",
      "number of sources available for training 100746\n",
      "Training on 100746... G: 82452, Q: 14042, S: 4252\n",
      "  f1score   [0.99100492 0.93748197 0.93469537]\n",
      " precision  [0.98725356 0.94946323 0.96752266]\n",
      "    recall     [0.99478491 0.92579933 0.90402258]\n",
      "------------------------------\n",
      "[0.99100492 0.93748197 0.93469537]\n"
     ]
    }
   ],
   "source": [
    "train_vs_f1score(df, place, sampleG=True)\n",
    "train_vs_f1score(df, place, sampleG=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66286470",
   "metadata": {},
   "source": [
    "## Cross Vadiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd4a6413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_prep_dict_all = prepare_data(df, feature_columns, train_percent=0.5)\n",
    "\n",
    "\n",
    "# # n_estimators\n",
    "# print('cross-validating...')\n",
    "# all_scores = []\n",
    "# for n_estimators in [20, 50, 100, 200, 500, 1000]:\n",
    "#     print(n_estimators)\n",
    "#     rfc = RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators, random_state=0, class_weight='balanced')\n",
    "#     scores = cross_validate(rfc, data_prep_dict_all['features_train'], data_prep_dict_all['classes_train'], scoring='f1_weighted', cv=5, n_jobs=-1, return_train_score=True)\n",
    "#     all_scores.append([n_estimators, scores])\n",
    "#     print('-'*30)\n",
    "    \n",
    "# print('cv_scores_leaf')\n",
    "# print(all_scores)\n",
    "\n",
    "\n",
    "\n",
    "# # max_features\n",
    "# print('cross-validating...')\n",
    "# all_scores = []\n",
    "# for feat in [2,3,4,5,6]:\n",
    "#     print(feat)\n",
    "#     rfc = RandomForestClassifier(n_jobs=-1, n_estimators=100, max_features=feat, random_state=0, class_weight='balanced')\n",
    "#     scores = cross_validate(rfc, data_prep_dict_all['features_train'], data_prep_dict_all['classes_train'], scoring='f1_weighted', cv=5, n_jobs=-1, return_train_score=True)\n",
    "#     all_scores.append([feat, scores])\n",
    "#     print('-'*30)\n",
    "    \n",
    "    \n",
    "    \n",
    "# # min_samples_leaf\n",
    "# print('cross-validating...')\n",
    "# all_scores = []\n",
    "# for leaf in [1,5, 10, 50, 100, 500]:\n",
    "#     print(leaf)\n",
    "#     rfc = RandomForestClassifier(n_jobs=-1, n_estimators=100, min_samples_leaf=leaf, random_state=0, class_weight='balanced')\n",
    "#     scores = cross_validate(rfc, data_prep_dict_all['features_train'], data_prep_dict_all['classes_train'], scoring='f1_weighted', cv=5, n_jobs=-1, return_train_score=True)\n",
    "#     all_scores.append([leaf, scores])\n",
    "#     print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb822c40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
